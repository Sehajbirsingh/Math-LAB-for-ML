{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:black;\"> Lab Exercise 6</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can download the original Kyphosis dataset from kaggle at https://www.kaggle.com/abbasit/kyphosis-dataset?select=kyphosis.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Kyphosis  Age  Number  Start\n",
      "0   absent   71       3      5\n",
      "1   absent  158       3     14\n",
      "2  present  128       4      5\n",
      "3   absent    2       5      1\n",
      "4   absent    1       4     15\n",
      "5   absent    1       2     16\n",
      "6   absent   61       2     17\n",
      "7   absent   37       3     16\n",
      "8   absent  113       2     16\n",
      "9  present   59       6     12\n"
     ]
    }
   ],
   "source": [
    "Kyphosis_df = pd.read_csv('kyphosis.csv')\n",
    "print(Kyphosis_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing features and target\n",
    "X = Kyphosis_df.drop('Kyphosis',axis=1)\n",
    "y = Kyphosis_df['Kyphosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDecisionTreeClassifier(\\n    criterion='gini',\\n    splitter='best',\\n    max_depth=None,\\n    min_samples_split=2,\\n    min_samples_leaf=1,\\n    min_weight_fraction_leaf=0.0,\\n    max_features=None,\\n    random_state=None,\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    class_weight=None,\\n    presort=False,\\n)\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting the right \"parameters/hyperparameters of ML models can significantly affect their performacnce\".\n",
    "# let's begin exploring those parameters more for a DT model.\n",
    "'''\n",
    "DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    random_state=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    class_weight=None,\n",
    "    presort=False,\n",
    ")\n",
    "'''\n",
    "# Let's consider these three parameters: criterion, splitter, min_samples_split\n",
    "    # for parameters consider two cases 'gini' and 'entropy'\n",
    "    # for splitter consider two cases 'best' and 'random'\n",
    "    # for min_samples_split consider two cases 2 and 5\n",
    "    # explain and dicuss your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(criterion, splitter, min_samples_split):\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        splitter=splitter,\n",
    "        min_samples_split=min_samples_split,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\n",
    "    ('gini', 'best', 2),\n",
    "    ('gini', 'best', 5),\n",
    "    ('gini', 'random', 2),\n",
    "    ('gini', 'random', 5),\n",
    "    ('entropy', 'best', 2),\n",
    "    ('entropy', 'best', 5),\n",
    "    ('entropy', 'random', 2),\n",
    "    ('entropy', 'random', 5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "for criterion, splitter, min_samples_split in combinations:\n",
    "    accuracy, report = train_and_evaluate(criterion, splitter, min_samples_split)\n",
    "    key = f\"criterion={criterion}, splitter={splitter}, min_samples_split={min_samples_split}\"\n",
    "    results[key] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: criterion=gini, splitter=best, min_samples_split=2\n",
      "Accuracy: 0.84\n",
      "--------------------------------------------------\n",
      "Parameters: criterion=gini, splitter=best, min_samples_split=5\n",
      "Accuracy: 0.8\n",
      "--------------------------------------------------\n",
      "Parameters: criterion=gini, splitter=random, min_samples_split=2\n",
      "Accuracy: 0.84\n",
      "--------------------------------------------------\n",
      "Parameters: criterion=gini, splitter=random, min_samples_split=5\n",
      "Accuracy: 0.68\n",
      "--------------------------------------------------\n",
      "Parameters: criterion=entropy, splitter=best, min_samples_split=2\n",
      "Accuracy: 0.8\n",
      "--------------------------------------------------\n",
      "Parameters: criterion=entropy, splitter=best, min_samples_split=5\n",
      "Accuracy: 0.76\n",
      "--------------------------------------------------\n",
      "Parameters: criterion=entropy, splitter=random, min_samples_split=2\n",
      "Accuracy: 0.68\n",
      "--------------------------------------------------\n",
      "Parameters: criterion=entropy, splitter=random, min_samples_split=5\n",
      "Accuracy: 0.76\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, accuracy in results.items():\n",
    "    print(f\"Parameters: {key}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best combination: criterion=gini, splitter=best, min_samples_split=2\n",
      "Best accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "best_combination = max(results, key=results.get)\n",
    "print(f\"\\nBest combination: {best_combination}\")\n",
    "print(f\"Best accuracy: {results[best_combination]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Performing Combination:\n",
    "The best performance was achieved with:\n",
    "\n",
    "criterion: gini\n",
    "splitter: best\n",
    "min_samples_split: 2\n",
    "This combination yielded an accuracy of 0.84 (84%).\n",
    "\n",
    "\n",
    "Impact of Parameters:\n",
    "a) Criterion (Gini vs Entropy):\n",
    "\n",
    "Gini generally performed better than Entropy.\n",
    "Gini's best accuracy: 0.84\n",
    "Entropy's best accuracy: 0.80\n",
    "This suggests that for this dataset, the Gini impurity measure was slightly more effective at creating meaningful splits.\n",
    "\n",
    "b) Splitter (Best vs Random):\n",
    "\n",
    "'Best' splitter generally outperformed 'Random'.\n",
    "'Best' achieved the highest accuracy of 0.84 multiple times.\n",
    "'Random' had more variable results, ranging from 0.68 to 0.84.\n",
    "This indicates that for this dataset, systematically choosing the best split is more reliable than random splitting.\n",
    "\n",
    "c) min_samples_split (2 vs 5):\n",
    "\n",
    "A value of 2 generally performed better than 5.\n",
    "With 2, we see the highest accuracies of 0.84.\n",
    "With 5, the highest accuracy drops to 0.80.\n",
    "This suggests that allowing the tree to create smaller splits (down to 2 samples) helps capture more nuanced patterns in this dataset.\n",
    "\n",
    "\n",
    "Consistency:\n",
    "The combination of Gini criterion with 'best' splitter seems most consistent, achieving 0.84 accuracy with both min_samples_split values.\n",
    "Worst Performing Combinations:\n",
    "The lowest accuracy (0.68) occurred twice:\n",
    "\n",
    "Gini, Random, min_samples_split=5\n",
    "Entropy, Random, min_samples_split=2\n",
    "This reinforces that random splitting can lead to suboptimal performance, especially when combined with other constraining factors.\n",
    "\n",
    "\n",
    "Overall Performance Range:\n",
    "Accuracies ranged from 0.68 to 0.84, a difference of 16 percentage points. This significant range demonstrates the importance of parameter tuning for decision trees.\n",
    "Robustness:\n",
    "The 'best' splitter seems more robust to changes in other parameters, consistently achieving higher accuracies compared to 'random'.\n",
    "\n",
    "Conclusions:\n",
    "\n",
    "For this Kyphosis dataset, using the Gini criterion with the 'best' splitter and a min_samples_split of 2 yields the best performance.\n",
    "The choice of splitter (best vs random) seems to have the most significant impact on performance.\n",
    "Allowing for smaller splits (min_samples_split=2) generally improves performance, suggesting the presence of some fine-grained patterns in the data.\n",
    "While Gini slightly outperforms Entropy, the difference is not drastic, indicating that both criteria are viable for this dataset.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "Use the best-performing combination (Gini, best, min_samples_split=2) for your final model.\n",
    "If model simplicity is a concern, you could opt for min_samples_split=5 with Gini and 'best' splitter, as it still performs well (0.80 accuracy) and might result in a less complex tree.\n",
    "Consider further tuning other parameters like max_depth or min_samples_leaf to potentially improve performance further.\n",
    "If possible, gather more data to see if it helps improve the model's accuracy, especially given the relatively small difference between some parameter combinations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
